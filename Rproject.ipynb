{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Rproject.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"gxEb4vQ7NBRu","colab_type":"text"},"source":["###JOURNAL SCRAPING PROJECT"]},{"cell_type":"markdown","metadata":{"id":"6dIHhAPj2VwG","colab_type":"text"},"source":["###Installing Newspaper Article libraries to Scrap web pages."]},{"cell_type":"code","metadata":{"id":"WDDUsUwONaJM","colab_type":"code","colab":{}},"source":["!pip install newspaper3k"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dqtgECGGUbdL","colab_type":"code","colab":{}},"source":["!apt-get install python3-bs4 "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zddGZuuyk1uL","colab_type":"code","colab":{}},"source":["import nltk\n","nltk.download('punkt')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"EhBvS3Tx2hR-","colab_type":"text"},"source":["###Getting Data from Beautiful Soap Which has libraries to scrap data"]},{"cell_type":"code","metadata":{"id":"aDv2PvgHUZKo","colab_type":"code","colab":{}},"source":["import httplib2\n","import nltk\n","from bs4 import BeautifulSoup, SoupStrainer\n","import requests \n","import csv \n","from newspaper import Article \n","from newspaper import fulltext\n","import pandas as pd\n","import numpy as np\n","import collections\n","articler,l = {},{}\n","#l ={}\n","cn = 0\n","http = httplib2.Http()\n","#calling URL  For Journal and Scrap each page of Journal year:\n","for i in range(1,52):\n","  t = 'https://gsejournal.biomedcentral.com/articles?tab=keyword&searchType=journalSearch&sort=PubDate&volume='\n","  r = i\n","  k,f= [],[]\n","  #f = []\n","  for j in range(0,9):\n","      s = f'&page={j}'\n","      status, response = http.request(f'{t}{r}{s}')\n","      for link in BeautifulSoup(response, 'html.parser', parseOnlyThese=SoupStrainer('a')):\n","        if link.has_attr('href'):\n","          k.append(link['href'])\n","  x = [item for item, count in collections.Counter(k).items() if count > 1]\n","  for p in x:\n","    if p.startswith('/articles/10.1186')==True:\n","      f.append(p)\n","      cn+=1\n","  articler[f'{i}'] = f\n","  l[f'{i}'] = 1968+i \n","  year = dict((l[key], value) for (key, value) in articler.items())\n","  "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s0ImOONz2p2j","colab_type":"text"},"source":["##Defination(Function) for calling  a year .User can input year into function and it will give all 8 Feilds."]},{"cell_type":"code","metadata":{"id":"ZpNulmCR72us","colab_type":"code","colab":{}},"source":["###Definatiion for scraping Data.\n","def volume(x):\n","  cn =0\n","  scrap = {}\n","  s1 = 'https://gsejournal.biomedcentral.com'\n","  for i in year.keys():\n","    if(i==x):\n","      for j in year[i]:\n","        m = 'https://'\n","        n = 'doi.org/'\n","        URL = f'{s1}{j}'\n","        r = requests.get(URL)\n","        article_name = Article(f'{s1}{j}', language=\"en\")\n","        article_name.download()\n","        #print(f'{s1}{j}')\n","        soup = BeautifulSoup(r.content, 'html5lib') \n","        table = soup.find('meta', attrs = {'name':'citation_abstract'})\n","        table1 = soup.find('meta', attrs = {'name':'prism.doi'})\n","        p = table1['content'][4:]\n","        article_name.parse()\n","        article_name.nlp()\n","        #print(article_name.publish_date)\n","        #print(article_name.keywords)\n","        #print(article_name.title) \n","        #print(\"DOI\",f'{m}{n}{p}',\"\\nArticle Name\\n\",article_name.title)\n","        auth = []\n","        for link in soup.find_all('meta',attrs = {'name':'citation_author'}):\n","          auth.append(link.get('content')) \n","        #print(\"corresponding author\",table4.string)\n","        aff= []\n","        for k in soup.find_all('span',itemprop='affiliation'):\n","          #print(k.find('meta')['content'])\n","          aff.append(k.find('meta',attrs={'itemprop':'address'})['content'])\n","          #print(k.find('meta',attrs={'itemprop':'address'})['content'])\n","        #print(\"email\",f'{s1}{tb5}')\n","        full = requests.get(URL).text\n","        #print(fulltext(full))\n","        scrap.setdefault('DOI',[]).append(f'{m}{n}{p}')\n","        scrap.setdefault('Title',[]).append(article_name.title)\n","        scrap.setdefault('Authors',[]).append(auth)\n","        scrap.setdefault('Author Affiliations',[]).append(aff)\n","        scrap.setdefault('Publication Date',[]).append(article_name.publish_date)\n","        scrap.setdefault('Abstract',[]).append(table['content'])\n","        scrap.setdefault('keywords',[]).append(article_name.keywords)\n","        scrap.setdefault('full text',[]).append(fulltext(full))\n","        cn+=1\n","  return scrap\n","\n","          "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jxsAhR1H21jf","colab_type":"text"},"source":["##Calling Function"]},{"cell_type":"code","metadata":{"id":"N3ZLkFlV_fKD","colab_type":"code","colab":{}},"source":["t = volume(1969)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0q04xrkF25kH","colab_type":"text"},"source":["##Defination for making a Text for input  Year"]},{"cell_type":"code","metadata":{"id":"ulWjf5mvh6xn","colab_type":"code","colab":{}},"source":["def makingtext(year):\n","  for i in range(1,52):  \n","    x = 1968+i\n","    if (year = x):\n","      t = volume(x)\n","      df = pd.DataFrame(t)\n","      p = 'Genetic'\n","      j = f'{p}{x}'+'.txt'\n","      export_csv = df.to_csv (f'{j}', index = None, header=True) #Don't forget to add '.csv' at the end of the path\n","    return export_csv\n","\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LhhluZl8AswU","colab_type":"text"},"source":["###ReadingText file\n"]},{"cell_type":"code","metadata":{"id":"xdhCs0F64H9b","colab_type":"code","colab":{}},"source":["files = pd.read_csv(makingtext(year),header=None)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PR6CtPkvHOlm","colab_type":"code","colab":{}},"source":["files"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XighBNbuA4lP","colab_type":"text"},"source":["###Making Directries for all Journal Year:to load HTML page"]},{"cell_type":"code","metadata":{"id":"Z1re3jC7bhEG","colab_type":"code","colab":{}},"source":["for i in range(1,52):\n","  x = 1968+i\n","  os.mkdir(f'{x}')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qKoVkhKnYcod","colab_type":"code","colab":{}},"source":["###Generating all Html page:\n","import urllib.request\n","for i in range(1,52):\n","  x = 1968+i\n","  t  = volume(x)\n","  df = pd.DataFrame(t)\n","  k = '/content/drive/My Drive/Rproject/Scrap/'\n","  m = f'{k}{x}'\n","  os.chdir(m)\n","  for i in range(len(df['DOI'])):\n","    try:\n","      htmlfile = urllib.request.urlopen(df['DOI'][i])\n","      htmltext = htmlfile.read().decode('utf-8')\n","      t =df['DOI'][i][16:23]+'-'+df['DOI'][i][24:]\n","      z = f'{t}'\n","      f  = open(z+'.html','w+')\n","      f.write(htmltext)\n","    except:\n","      pass\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ies46RAMA-Hy","colab_type":"text"},"source":["### NO coresponding Author For this Data Couple of Article have a correspond's Author after 2001 so defination for coresponding author and email."]},{"cell_type":"code","metadata":{"id":"-j8gPUMlCkwT","colab_type":"code","colab":{}},"source":["      def corespondemce(x):\n","        cn =0\n","        scrap = {}\n","        s1 = 'https://gsejournal.biomedcentral.com'\n","        for i in year.keys():\n","          if(i==x):\n","            for j in year[i]:\n","              m = 'https://'\n","              n = 'doi.org/'\n","              URL = f'{s1}{j}'\n","              r = requests.get(URL)\n","              try:\n","                soup = BeautifulSoup(r.content, 'html5lib') \n","                article_name = Article(f'{s1}{j}', language=\"en\")\n","                article_name.download()\n","                table4 = soup.find('a',attrs={'id':'corresp-c1'})\n","                table5 = soup.find('a',attrs={'id':'corresp-c1'})\n","                tb5 = table5['href']\n","                scrap.setdefault('Corresponding Author',[]).append(table4.string)\n","                scrap.setdefault('Corresponding Author s Email',[]).append(f'{s1}{tb5}')\n","              except:\n","                print(\"There is no Corespondece for this year\")\n","            return scrap\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kr2mgWMi5z4U","colab_type":"code","colab":{}},"source":["corespondemce(1969)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5FA34Bz1BJfJ","colab_type":"text"},"source":["####THANK YOU#####\n"]}]}